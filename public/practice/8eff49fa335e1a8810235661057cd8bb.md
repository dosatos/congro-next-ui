
## Distributed Web Crawler

**Question:** Design a distributed web crawler for a search engine

**Hints:**
- Consider implementing a distributed URL frontier
- Think about how to respect robots.txt and implement crawl delays
- Explore options for efficient content extraction and parsing
- Consider how to handle duplicate content and near-duplicate detection
- Think about implementing a distributed indexing system for crawled content

> #### Key Considerations:
> - URL frontier management
> - Politeness and rate limiting
> - Content extraction and indexing
